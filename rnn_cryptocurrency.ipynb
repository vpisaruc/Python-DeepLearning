{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_cryptocurrency.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVWcYU6MJJQa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f10c444c-2101-4e04-a62f-9509ea88b506"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA3e4q8wJVjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp drive/'My Drive'/'Colab Notebooks'/Data/crypto_data/BTC-USD.csv .\n",
        "!cp drive/'My Drive'/'Colab Notebooks'/Data/crypto_data/BCH-USD.csv .\n",
        "!cp drive/'My Drive'/'Colab Notebooks'/Data/crypto_data/LTC-USD.csv .\n",
        "!cp drive/'My Drive'/'Colab Notebooks'/Data/crypto_data/ETH-USD.csv ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2V5MsETJX3_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "82d53cf1-a4e4-4cf2-f318-ce39a7bdd010"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn import preprocessing\n",
        "from collections import deque\n",
        "import time \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "\n",
        "\n",
        "# we will use last 60 minits of our data to make predictions\n",
        "SEQ_LEN = 60\n",
        "# period in minits\n",
        "FUTURE_PERIOD_PREDICT = 3\n",
        "RATIO_TO_PREDICT = 'ETH-USD'\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "NAME = f'{RATIO_TO_PREDICT}-{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}'\n",
        "\n",
        "\n",
        "def classify(current, future):\n",
        "    # if the price in the future greater then now return 1\n",
        "    # 1 - is buy\n",
        "    if float(future) > float(current):\n",
        "        return 1\n",
        "    # 0 - is sell\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# preparing our data for recurrent neural network\n",
        "def preprocess_df(df):\n",
        "    df = df.drop('future', 1)\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col != 'target':\n",
        "            df[col] = df[col].pct_change()\n",
        "            df.dropna(inplace=True)\n",
        "            df[col] = preprocessing.scale(df[col].values)\n",
        "\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    sequential_data = []\n",
        "    # make queue from list with max len when it reaches the max len it pops the \n",
        "    # old items \n",
        "    prev_days = deque(maxlen=SEQ_LEN) \n",
        "\n",
        "    for i in df.values:\n",
        "        prev_days.append([n for n in i[:-1]])\n",
        "        if len(prev_days) == SEQ_LEN:\n",
        "            sequential_data.append([np.array(prev_days), i[-1]])\n",
        "\n",
        "    random.shuffle(sequential_data)\n",
        "\n",
        "    buys = []\n",
        "    sells = []\n",
        "\n",
        "    for seq, target in sequential_data:\n",
        "        if target == 0:\n",
        "            sells.append([seq, target])\n",
        "        elif target == 1:\n",
        "            buys.append([seq, target])\n",
        "        \n",
        "    random.shuffle(buys)\n",
        "    random.shuffle(sells)\n",
        "\n",
        "    lower = min(len(buys), len(sells))\n",
        "\n",
        "    buys = buys[:lower]\n",
        "    sells = sells[:lower]\n",
        "\n",
        "    sequential_data = buys + sells\n",
        "    random.shuffle(sequential_data)\n",
        "\n",
        "    X  = []\n",
        "    y = []\n",
        "\n",
        "    for seq, target in sequential_data:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "\n",
        "    return np.array(X), y\n",
        "\n",
        "\n",
        "df = pd.read_csv('LTC-USD.csv', \n",
        "                 names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
        "\n",
        "main_df = pd.DataFrame()\n",
        "\n",
        "ratios = ['BTC-USD', 'LTC-USD', 'ETH-USD', 'BCH-USD']\n",
        "for ratio in ratios:\n",
        "    dataset = f'{ratio}.csv'\n",
        "\n",
        "    df = pd.read_csv(dataset, \n",
        "                     names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
        "\n",
        "    df.rename(columns={'close' : f'{ratio}_close', \n",
        "                       'volume': f'{ratio}_volume'}, \n",
        "              inplace=True)  \n",
        "\n",
        "    df.set_index('time', inplace=True)\n",
        "    df = df[[f'{ratio}_close', f'{ratio}_volume']]   \n",
        "\n",
        "    if len(main_df) == 0:\n",
        "        main_df = df\n",
        "    else:\n",
        "        main_df = main_df.join(df)\n",
        "\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "\n",
        "\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'],\n",
        "                             main_df['future']))\n",
        "\n",
        "times = sorted(main_df.index.values)\n",
        "last_5pct = times[-int(0.05 * len(times))]\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
        "main_df = main_df[(main_df.index < last_5pct)]\n",
        "\n",
        "train_x, train_y = preprocess_df(main_df)\n",
        "validation_x, validation_y = preprocess_df(validation_main_df)\n",
        "\n",
        "print(f'train data: {len(train_x)} validation: {len(validation_x)}')\n",
        "print(f'Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}')\n",
        "print(f'VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(CuDNNLSTM(128, input_shape=(train_x.shape[1:]), \n",
        "                    return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(CuDNNLSTM(128, input_shape=(train_x.shape[1:]), \n",
        "                    return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(CuDNNLSTM(128, input_shape=(train_x.shape[1:])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=f'logs/{NAME}')\n",
        "\n",
        "filepath = 'RNN_Final-{epoch:02d}-{val_acc:.3f}'\n",
        "checkpoint = ModelCheckpoint('{}.model'.format(filepath), \n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             model='max')\n",
        "\n",
        "history = model.fit(\n",
        "          train_x, train_y,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          validation_data=(validation_x, validation_y),\n",
        "          callbacks=[tensorboard, checkpoint])\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data: 74196 validation: 3260\n",
            "Dont buys: 37098, buys: 37098\n",
            "VALIDATION Dont buys: 1630, buys: 1630\n",
            "Train on 74196 samples, validate on 3260 samples\n",
            "Epoch 1/10\n",
            "74048/74196 [============================>.] - ETA: 0s - loss: 0.7103 - acc: 0.5160\n",
            "Epoch 00001: val_acc improved from -inf to 0.52791, saving model to RNN_Final-01-0.528.model\n",
            "74196/74196 [==============================] - 24s 318us/sample - loss: 0.7103 - acc: 0.5158 - val_loss: 0.6909 - val_acc: 0.5279\n",
            "Epoch 2/10\n",
            "74176/74196 [============================>.] - ETA: 0s - loss: 0.6891 - acc: 0.5353\n",
            "Epoch 00002: val_acc improved from 0.52791 to 0.54080, saving model to RNN_Final-02-0.541.model\n",
            "74196/74196 [==============================] - 21s 285us/sample - loss: 0.6891 - acc: 0.5353 - val_loss: 0.6891 - val_acc: 0.5408\n",
            "Epoch 3/10\n",
            "74176/74196 [============================>.] - ETA: 0s - loss: 0.6866 - acc: 0.5471\n",
            "Epoch 00003: val_acc improved from 0.54080 to 0.55675, saving model to RNN_Final-03-0.557.model\n",
            "74196/74196 [==============================] - 21s 283us/sample - loss: 0.6866 - acc: 0.5471 - val_loss: 0.6836 - val_acc: 0.5567\n",
            "Epoch 4/10\n",
            "74112/74196 [============================>.] - ETA: 0s - loss: 0.6844 - acc: 0.5526\n",
            "Epoch 00004: val_acc improved from 0.55675 to 0.56411, saving model to RNN_Final-04-0.564.model\n",
            "74196/74196 [==============================] - 21s 285us/sample - loss: 0.6844 - acc: 0.5526 - val_loss: 0.6810 - val_acc: 0.5641\n",
            "Epoch 5/10\n",
            "74048/74196 [============================>.] - ETA: 0s - loss: 0.6836 - acc: 0.5565\n",
            "Epoch 00005: val_acc did not improve from 0.56411\n",
            "74196/74196 [==============================] - 21s 283us/sample - loss: 0.6836 - acc: 0.5566 - val_loss: 0.6819 - val_acc: 0.5629\n",
            "Epoch 6/10\n",
            "74176/74196 [============================>.] - ETA: 0s - loss: 0.6821 - acc: 0.5606\n",
            "Epoch 00006: val_acc did not improve from 0.56411\n",
            "74196/74196 [==============================] - 21s 282us/sample - loss: 0.6821 - acc: 0.5606 - val_loss: 0.6855 - val_acc: 0.5620\n",
            "Epoch 7/10\n",
            "74112/74196 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.5639\n",
            "Epoch 00007: val_acc did not improve from 0.56411\n",
            "74196/74196 [==============================] - 21s 282us/sample - loss: 0.6806 - acc: 0.5638 - val_loss: 0.6818 - val_acc: 0.5592\n",
            "Epoch 8/10\n",
            "74048/74196 [============================>.] - ETA: 0s - loss: 0.6785 - acc: 0.5685\n",
            "Epoch 00008: val_acc did not improve from 0.56411\n",
            "74196/74196 [==============================] - 21s 284us/sample - loss: 0.6785 - acc: 0.5684 - val_loss: 0.6887 - val_acc: 0.5546\n",
            "Epoch 9/10\n",
            "74048/74196 [============================>.] - ETA: 0s - loss: 0.6750 - acc: 0.5774\n",
            "Epoch 00009: val_acc did not improve from 0.56411\n",
            "74196/74196 [==============================] - 21s 282us/sample - loss: 0.6750 - acc: 0.5775 - val_loss: 0.6874 - val_acc: 0.5485\n",
            "Epoch 10/10\n",
            "74048/74196 [============================>.] - ETA: 0s - loss: 0.6703 - acc: 0.5869\n",
            "Epoch 00010: val_acc did not improve from 0.56411\n",
            "74196/74196 [==============================] - 21s 283us/sample - loss: 0.6703 - acc: 0.5868 - val_loss: 0.6918 - val_acc: 0.5494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgazVkKIJX2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLTVIlKuJXyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0KlcWp2JXtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh9AYa6-JXrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yDwgOOEJXpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3CqekR-JXm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLyqkvzHJXgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}